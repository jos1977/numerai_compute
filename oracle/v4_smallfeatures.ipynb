{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d39aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "\n",
    "from numerapi import NumerAPI\n",
    "from utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    get_biggest_change_features,\n",
    "    validation_metrics,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL\n",
    ")\n",
    "\n",
    "public_id = \"FILLTHISIN\"\n",
    "secret_key = \"FILLTHISIN\"\n",
    "napi = NumerAPI(public_id=public_id, secret_key=secret_key,verbosity='info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebcbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if napi.check_new_round():\n",
    "    print(\"new round has started within the last 24hours!\")\n",
    "    \n",
    "else:\n",
    "    print(\"no new round within the last 24 hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_round = napi.get_current_round()\n",
    "previous_round = current_round - 1\n",
    "logging.info(\"Latest numerai dataset number is: %s'\", current_round)\n",
    "logging.info(\"Previous numerai dataset number is: %s'\", previous_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if all models already have submitted\n",
    "example_model_id = napi.get_models()['FILLTHISIN']\n",
    "#example_model2_id = napi.get_models()['FILLTHISIN']\n",
    "\n",
    "# check submission status\n",
    "if( napi.submission_status(example_model_id) ):\n",
    "    example_model_submitted = True\n",
    "else:\n",
    "    example_model_submitted = False\n",
    "\n",
    "# check submission status\n",
    "#if( napi.submission_status(example_model2_id) ):\n",
    "#    example_model2_submitted = True\n",
    "#else:\n",
    "#    example_model2_submitted = False\n",
    "\n",
    "if example_model_submitted:  #and example_model2_submtted: \n",
    "    allmodels_submitted = True\n",
    "else:\n",
    "    allmodels_submitted = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982caafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = \"v4\"\n",
    "\n",
    "CURRENT_TRAININGDATA = SRC + \"/train.parquet\"\n",
    "CURRENT_LIVEDATA = SRC + f\"/live_{current_round}.parquet\"\n",
    "CURRENT_VALIDATIONDATA = SRC + \"/validation.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General. minimal training data models based on the small dataset. Change the feature_set here or use your own feature sets (selection/engineering)\n",
    "if not allmodels_submitted:\n",
    "    print('Reading minimal training data')\n",
    "    # read the feature metadata and get a feature set (or all the features)\n",
    "    with open(\"v4/features.json\", \"r\") as f:\n",
    "        feature_metadata = json.load(f)\n",
    "    features = feature_metadata[\"feature_sets\"][\"small\"] # get the small feature set\n",
    "    read_columns = features + [ERA_COL, DATA_TYPE_COL, TARGET_COL]\n",
    "\n",
    "    #load data\n",
    "    logging.info(\"loading data\")\n",
    "    training_data = pd.read_parquet(f'v4/train.parquet', columns=read_columns)\n",
    "    live_data = pd.read_parquet(CURRENT_LIVEDATA, columns=read_columns)\n",
    "\n",
    "    # getting the per era correlation of each feature vs the target\n",
    "    all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
    "        lambda era: era[features].corrwith(era[TARGET_COL])\n",
    "    )\n",
    "\n",
    "    # find the riskiest features by comparing their correlation vs\n",
    "    # the target in each half of training data; we'll use these later\n",
    "    riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n",
    "\n",
    "    nans_per_col = live_data[live_data[\"data_type\"] == \"live\"][features].isna().sum()\n",
    "\n",
    "    # check for nans and fill nans\n",
    "    if nans_per_col.any():\n",
    "        total_rows = len(live_data[live_data[\"data_type\"] == \"live\"])\n",
    "        print(f\"Number of nans per column this week: {nans_per_col[nans_per_col > 0]}\")\n",
    "        print(f\"out of {total_rows} total rows\")\n",
    "        print(f\"filling nans with 0.5\")\n",
    "        live_data.loc[:, features] = live_data.loc[:, features].fillna(0.5)\n",
    "    else:\n",
    "        print(\"No nans in the features this week!\")\n",
    "else:\n",
    "    print(\"All models already submitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example_Model, this should be customized and extended to your own model(s)\n",
    "if not example_model_submitted:\n",
    "    \n",
    "    PREDICTION_FILE = \"./predictions/prediction_examplemodel_v4_\" + str(current_round) + \".csv\"\n",
    "    PREVIOUS_PREDICTION_FILE = \"./predictions/prediction_examplemodel_v4_\" + str(previous_round) + \".csv\"\n",
    "    MODEL_FILE = \"./models/examplemodel_v4/model.pkl\"\n",
    "\n",
    "    logging.info(\"PREDICTION_FILE is: %s\", PREDICTION_FILE)\n",
    "    logging.info(\"PREVIOUS PREDICTION FILE is: %s\", PREVIOUS_PREDICTION_FILE)\n",
    "    logging.info(\"MODEL_FILE is: %s\", MODEL_FILE)\n",
    "\n",
    "    if(os.path.isfile(PREVIOUS_PREDICTION_FILE)):\n",
    "        os.remove(PREVIOUS_PREDICTION_FILE)\n",
    "        #Printing the confirmation message of deletion\n",
    "        print(\"File Deleted successfully\")\n",
    "    else:\n",
    "        print(\"File does not exist\")\n",
    "    \n",
    "    model = joblib.load(MODEL_FILE)\n",
    "\n",
    "    live_data[\"preds\"] = model.predict(live_data[features])\n",
    "\n",
    "    live_data[f\"preds_neutral_riskiest_50\"] = neutralize(\n",
    "        df=live_data,\n",
    "        columns=[f\"preds\"],\n",
    "        neutralizers=riskiest_features,\n",
    "        proportion=1.0,\n",
    "        normalize=True,\n",
    "        era_col=ERA_COL\n",
    "    )\n",
    "\n",
    "    # rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "    live_data[\"prediction\"] = live_data[f\"preds_neutral_riskiest_50\"].rank(pct=True)\n",
    "    live_data[\"prediction\"].to_csv(PREDICTION_FILE)\n",
    "    \n",
    "    i = 1\n",
    "    while i < 3:    \n",
    "        try:\n",
    "            submission_id = napi.upload_predictions(PREDICTION_FILE, model_id=example_model_id)\n",
    "            time.sleep(3)\n",
    "            i = 3\n",
    "            print(\"submitted example_model\")\n",
    "        except:\n",
    "            time.sleep(10) # Sleep for 10 seconds\n",
    "            i += 1\n",
    "else:\n",
    "    print(\"example_model already submitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478eae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example_Model, this should be customized and extended to your own model(s)\n",
    "# if not example_model2_submitted:\n",
    "    \n",
    "#     PREDICTION_FILE = \"./predictions/prediction_examplemodel2_v4_\" + str(current_round) + \".csv\"\n",
    "#     PREVIOUS_PREDICTION_FILE = \"./predictions/prediction_examplemodel2_v4_\" + str(previous_round) + \".csv\"\n",
    "#     MODEL_FILE = \"./models/examplemodel2_v4/model.pkl\"\n",
    "\n",
    "#     logging.info(\"PREDICTION_FILE is: %s\", PREDICTION_FILE)\n",
    "#     logging.info(\"PREVIOUS PREDICTION FILE is: %s\", PREVIOUS_PREDICTION_FILE)\n",
    "#     logging.info(\"MODEL_FILE is: %s\", MODEL_FILE)\n",
    "\n",
    "#     if(os.path.isfile(PREVIOUS_PREDICTION_FILE)):\n",
    "#         os.remove(PREVIOUS_PREDICTION_FILE)\n",
    "#         #Printing the confirmation message of deletion\n",
    "#         print(\"File Deleted successfully\")\n",
    "#     else:\n",
    "#         print(\"File does not exist\")\n",
    "    \n",
    "#     model = joblib.load(MODEL_FILE)\n",
    "\n",
    "#     live_data[\"preds\"] = model.predict(live_data[features])\n",
    "\n",
    "#     live_data[f\"preds_neutral_riskiest_50\"] = neutralize(\n",
    "#         df=live_data,\n",
    "#         columns=[f\"preds\"],\n",
    "#         neutralizers=riskiest_features,\n",
    "#         proportion=1.0,\n",
    "#         normalize=True,\n",
    "#         era_col=ERA_COL\n",
    "#     )\n",
    "\n",
    "#     # rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "#     live_data[\"prediction\"] = live_data[f\"preds_neutral_riskiest_50\"].rank(pct=True)\n",
    "#     live_data[\"prediction\"].to_csv(PREDICTION_FILE)\n",
    "    \n",
    "#     i = 1\n",
    "#     while i < 3:    \n",
    "#         try:\n",
    "#             submission_id = napi.upload_predictions(PREDICTION_FILE, model_id=example_model2_id)\n",
    "#             time.sleep(3)\n",
    "#             i = 3\n",
    "#             print(\"submitted example_model2\")\n",
    "#         except:\n",
    "#             time.sleep(10) # Sleep for 10 seconds\n",
    "#             i += 1\n",
    "# else:\n",
    "#     print(\"example_model2 already submitted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "59b3c14b51397012e7ef1f702b0c53b9aaa3ddacfed4f237adf082bfa05232e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
